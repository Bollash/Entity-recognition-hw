{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Entity recognition.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Bollash/Entity-recognition-hw/blob/main/Entity_recognition.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qs-CZblTLLVp"
      },
      "source": [
        " !pip install transformers"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aHJIXfw_4fk8"
      },
      "source": [
        "import pandas as pd\n",
        "import urllib.request\n",
        "import gzip\n",
        "import shutil\n",
        "import numpy as np\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from sklearn.model_selection import train_test_split\n",
        "from transformers import AutoTokenizer"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1HPGeP9I96kx"
      },
      "source": [
        "#Downloading the dataset.\n",
        "url = \"http://hlt.sztaki.hu/resources/hunnerwiki/huwiki.1.ner.tsv.gz\"\n",
        "local_file = \"data.tsv.gz\"\n",
        "urllib.request.urlretrieve(url, local_file)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vp7tYTYV_djp"
      },
      "source": [
        "#Decompressing the data\n",
        "with gzip.open(local_file, 'rb') as f_in:\n",
        "    with open('file.tsv', 'wb') as f_out:\n",
        "        shutil.copyfileobj(f_in, f_out)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OZc6m9C1FzsD"
      },
      "source": [
        "#Loading the data into data_set. There are lines that produce errors. We filter them out by using the third parameter\n",
        "data_set = pd.read_csv('file.tsv', sep='\\t', error_bad_lines=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LvLf3ACRHN08"
      },
      "source": [
        "data_set.head"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_3aAIBz6MPYA"
      },
      "source": [
        "#Drop the empty lines\n",
        "data_set = data_set.dropna()\n",
        "\n",
        "smaller = data_set[['A','O']]\n",
        "#Using a fraction of the original data since there are 2.3 million elements.\n",
        "smaller = smaller[0:200000]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0H2nU7hzcGAJ"
      },
      "source": [
        "#Some lines are NaN. We filter them out this way.\n",
        "smaller = smaller[smaller['O'].notnull()]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I_Ig1HT1Z5nh"
      },
      "source": [
        "#To convert the tags into numbers we'll use a dictionary. Currently there is no 'PAD', but we shall add them later.\n",
        "d = {\n",
        "    'B-LOC' : 0,\n",
        "    'B-MISC' : 1,\n",
        "    'B-ORG' : 2,\n",
        "    'B-PER' : 3,\n",
        "    'I-LOC' : 4,\n",
        "    'I-MISC' : 5,\n",
        "    'I-ORG' : 6,\n",
        "    'I-PER' : 7,\n",
        "    'O' : 8,\n",
        "    'PAD' : 9,\n",
        "    'BOS' : 10,\n",
        "    'EOS' : 11\n",
        "}\n",
        "#Converting the tags\n",
        "for idx, row in smaller.iterrows():\n",
        "  row[1] = d[row[1]]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uK7pCmTaOGew"
      },
      "source": [
        "#Reconstructing the sentences.\n",
        "\n",
        "#Used to filter out punctuation.\n",
        "garbage = {'.', ',', '\\\"', \"\\'\", '/', '\\\\', '(', ')', '\\'', ':', '?', '!', 'â€™', '-', ';'}\n",
        "sentences = []\n",
        "y_sentences = []\n",
        "sent = []\n",
        "y_sent = []\n",
        "for idx, row in smaller.iterrows():\n",
        "  #Theese characters mark the end of a sentence.\n",
        "  if row[0] in {'.', '?', '!', ':'}:\n",
        "    sentences.append(sent)\n",
        "    sent = []\n",
        "    y_sentences.append(y_sent)\n",
        "    y_sent = []\n",
        "    continue\n",
        "  if row[0] not in garbage:\n",
        "    sent.append(row[0])\n",
        "    y_sent.append(row[1])\n",
        "  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EEm6-cpRQ9yq"
      },
      "source": [
        "#We're usning hubert base cc tokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"SZTAKI-HLT/hubert-base-cc\")\n",
        "\n",
        "def tokenize_sentence(sentence, tokenizer):\n",
        "  tokenized = []\n",
        "  for word in sentence:\n",
        "    tokenized.append(tokenizer.tokenize(word))\n",
        "  return tokenized"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BiwqRWVQVsHc"
      },
      "source": [
        "#Tokenize the input sentences.\n",
        "#We can't use the lemmas because Budapest is an entity but budapesti isn't.\n",
        "#This makes it, that most of our words consist of 1 token.\n",
        "\n",
        "max_word_len = 0\n",
        "tokenized_sentences = []\n",
        "for sent in sentences:\n",
        "  tokenized_sent = tokenize_sentence(sent, tokenizer)\n",
        "  for tok_word in tokenized_sent:\n",
        "    max_word_len = max(max_word_len, len(tok_word))\n",
        "  tokenized_sentences.append(tokenized_sent)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mSOTqR0ZTN5B"
      },
      "source": [
        "#Inserting a BOS(beginning of sentence) and EOS(end of sentence) token. It's needed for the ngrams.\n",
        "for sentence in tokenized_sentences:\n",
        "  sentence.insert(0, ['BOS'])\n",
        "  sentence.append(['EOS'])\n",
        "#Inserting BOS and EOS values to the output values aswell\n",
        "for y in y_sentences:\n",
        "  y.insert(0, 10)\n",
        "  y.append(11)\n",
        "\n",
        "#Padding the sentences\n",
        "for sentence in tokenized_sentences:\n",
        "  for word in sentence:\n",
        "    for _ in range(max_word_len - len(word)):\n",
        "      word.append('PAD')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hSoro1kw_yWX"
      },
      "source": [
        "#Instead of the sentences we use ngrams. It makes it so theres less padding needed, and also we can use ngram based accuracy.\n",
        "\n",
        "def make_ngrams(sentence, n):\n",
        "  ngrams = []\n",
        "  for i in range(len(sentence) - n + 1):\n",
        "    ngram = []\n",
        "    for j in range(n):\n",
        "      ngram.append(sentence[i + j])\n",
        "    ngrams.append(ngram)\n",
        "  return ngrams"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WMp9yg0TAuoD"
      },
      "source": [
        "#Currently using bigramms, but it can be changed upwards if needed.\n",
        "ngram_size = 2\n",
        "ngrams = []\n",
        "for sentence in tokenized_sentences:\n",
        "  ngrams.append(make_ngrams(sentence, ngram_size))\n",
        "\n",
        "#When we convert to ngrams, theres one less ngram than the input size. So we need to cut the last output element.\n",
        "for y_sentence in y_sentences:\n",
        "  for _ in range(ngram_size - 1):\n",
        "    y_sentence.pop()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NfUrykqx3OGe"
      },
      "source": [
        "#One-Hot encode our tags\n",
        "for i in range(len(y_sentences)):\n",
        "  y_sentences[i] = to_categorical(y_sentences[i], 12)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GfUovl3UFuMc"
      },
      "source": [
        "#Flatten our data, so it's easier to work with.\n",
        "\n",
        "x_ngrams = []\n",
        "for sentence in ngrams:\n",
        "  for ngram in sentence:\n",
        "    x = []\n",
        "    for word in ngram:\n",
        "      for token in word:\n",
        "        x.append(token)\n",
        "    x_ngrams.append(x)\n",
        "\n",
        "y_ngrams = []\n",
        "for sentence in y_sentences:\n",
        "  for ngram in sentence:\n",
        "    y_ngrams.append(ngram)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xZQCc9c9oxXF"
      },
      "source": [
        "#We use a BoW(bag of words) method for the word to number conversion. An embedding layer could be used instead if we need a better accuracy.\n",
        "\n",
        "#Filling our vocab with the tokens.\n",
        "#Currently even the test data is put into it. It would be better no not include them and make an universal number for the tokens that are not inculded in the vocab.\n",
        "vocab, index = {}, 1  # start indexing from 1\n",
        "vocab['PAD'] = 0  # add a padding token\n",
        "for word in x_ngrams:\n",
        "  for token in word:\n",
        "    if token not in vocab:\n",
        "      vocab[token] = index\n",
        "      index += 1\n",
        "# 'Out of vocabulary' for unknown words\n",
        "vocab['OOV'] = index\n",
        "\n",
        "vocab_size = len(vocab)\n",
        "print(vocab)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "THVJXI6hTg91"
      },
      "source": [
        "#Converting our input into integers, with our BoW vocab.\n",
        "\n",
        "embedded_x_ngrams = []\n",
        "for ngram in x_ngrams:\n",
        "  embedded_x_ngram = []\n",
        "  for token in ngram:\n",
        "    embedded_x_ngram.append(vocab[token])\n",
        "  embedded_x_ngrams.append(embedded_x_ngram)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8Hhx9V6HPxjn"
      },
      "source": [
        "#Converting the lists into Numpy arrays\n",
        "sentences = np.array(embedded_x_ngrams)\n",
        "y_sentences = np.array(y_ngrams)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CrEeF7Lh6vRg"
      },
      "source": [
        "#Split the data into train, test, and val data\n",
        "#train : 0.6\n",
        "#test : 0.2\n",
        "#val: 0.2\n",
        "#Since the train test split splits the data into 2 parts we have to call it twice\n",
        "x_train, x_test, y_train, y_test = train_test_split(sentences, y_sentences, test_size=0.2, random_state=123)\n",
        "#We need test_size=0.25 since 0.2 / 0.8 == 0.25\n",
        "x_train, x_val, y_train, y_val = train_test_split(x_train, y_train, test_size=0.25, random_state=123)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dCwN_gFMB619"
      },
      "source": [
        "print(x_train.shape)\n",
        "print(y_train.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4CpCGTK9Bpba"
      },
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jxh4VKTqDBeJ"
      },
      "source": [
        "input_shape = (28,)\n",
        "max_features = vocab_size\n",
        "embedding_size = 64\n",
        "lstm1_size = 256\n",
        "lstm2_size = 256\n",
        "optim = \"adam\"\n",
        "batch_size = 256\n",
        "inputs = keras.Input(shape=input_shape, dtype=\"int32\")\n",
        "\n",
        "x = layers.Embedding(max_features, embedding_size)(inputs)\n",
        "\n",
        "x = layers.Bidirectional(layers.LSTM(lstm1_size, return_sequences=True))(x)\n",
        "x = layers.Bidirectional(layers.LSTM(lstm2_size))(x)\n",
        "\n",
        "outputs = layers.Dense(12, activation=\"softmax\")(x)\n",
        "model = keras.Model(inputs, outputs)\n",
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.callbacks import EarlyStopping\n",
        "callbacks = [EarlyStopping(monitor='val_accuracy', patience=5, verbose=0)]"
      ],
      "metadata": {
        "id": "p9zxgsms_Lh8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "69cn2sY4Fdyq"
      },
      "source": [
        "model.compile(optim, \"categorical_crossentropy\", metrics=[\"accuracy\"])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-rYURjo7GFBb"
      },
      "source": [
        "\n",
        "model.fit(x_train, y_train, batch_size=batch_size, epochs=100, callbacks=callbacks, validation_data=(x_val, y_val))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uPV6ZJ0D-BTo"
      },
      "source": [
        "prediction = model.predict(x_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HGWe3ieubmus"
      },
      "source": [
        "def compare(pred, origi):\n",
        "  return np.argmax(pred) == np.argmax(origi)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r9ryvUCydLI9"
      },
      "source": [
        "#Checking the accuracy on the test data\n",
        "correct_predictions = 0\n",
        "for i in range(len(prediction)):\n",
        "  if(compare(prediction[i], y_test[i])):\n",
        "    correct_predictions += 1\n",
        "\n",
        "print(f\"Accuracy on the test data is: {correct_predictions / len(prediction):.4f}%\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#We have to look at the prediction, to see if theres some anomaly there.\n",
        "values = [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
        "for pred in prediction:\n",
        "  idx = np.argmax(pred)\n",
        "  values[idx] += 1\n",
        "\n",
        "#As we can see nearly all the values get predicted. Only EoS and Pad is missing, but that was expected.\n",
        "values"
      ],
      "metadata": {
        "id": "TDPC-cHNm-Fs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Demonstrating the results with sentences out of original dataset\n",
        "sample_sentences=[['A', 'tudomÃ¡ny', 'szerint', 'is', 'mÅ±kÃ¶dik', 'az', 'alvÃ¡smÃ³dszer', 'amit', 'Salvador', 'DalÃ­', 'is', 'hasznÃ¡lt'], ['Ronaldo', 'megint', 'meccset', 'nyert', 'a', 'Manchester', 'Unitednek'], ['Ã–t', 'ok', 'amiÃ©rt', 'Macron', 'Budapestre', 'lÃ¡togat']]\n",
        "sample_y_sentences=[[8,8,8,8,8,8,8,8,3,7,8,8],[3,8,8,8,8,3,7],[8,8,8,3,0,8]]"
      ],
      "metadata": {
        "id": "S1TzOjh-_4aS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# We are doing the same process as we did it with the original dataset in the beginning of the notebook\n",
        "sample_tokenized_sentences = []\n",
        "for sent in sample_sentences:\n",
        "  sample_tokenized_sentences.append(tokenize_sentence(sent, tokenizer))"
      ],
      "metadata": {
        "id": "5GPj8ouD_4lT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for sentence in sample_tokenized_sentences:\n",
        "  sentence.insert(0, ['BOS'])\n",
        "  sentence.append(['EOS'])\n",
        "\n",
        "for y in sample_y_sentences:\n",
        "  y.insert(0, 10)\n",
        "  y.append(11)\n",
        "\n",
        "for sentence in sample_tokenized_sentences:\n",
        "  for word in sentence:\n",
        "    for _ in range(max_word_len - len(word)):\n",
        "      word.append('PAD')"
      ],
      "metadata": {
        "id": "WESGuaYs_4nx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sample_ngrams = []\n",
        "for sentence in sample_tokenized_sentences:\n",
        "  sample_ngrams.append(make_ngrams(sentence, ngram_size))\n",
        "\n",
        "for y_sentence in sample_y_sentences:\n",
        "  for _ in range(ngram_size - 1):\n",
        "    y_sentence.pop()"
      ],
      "metadata": {
        "id": "0ktsQw7R_4rU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(len(sample_y_sentences)):\n",
        "  sample_y_sentences[i] = to_categorical(sample_y_sentences[i], 12)"
      ],
      "metadata": {
        "id": "ozMlteLwALx9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sample_x_ngrams = []\n",
        "for sentence in sample_ngrams:\n",
        "  for ngram in sentence:\n",
        "    x = []\n",
        "    for word in ngram:\n",
        "      for token in word:\n",
        "        x.append(token)\n",
        "    sample_x_ngrams.append(x)\n",
        "\n",
        "sample_y_ngrams = []\n",
        "for sentence in sample_y_sentences:\n",
        "  for ngram in sentence:\n",
        "    sample_y_ngrams.append(ngram)"
      ],
      "metadata": {
        "id": "laFXhqFjAQSP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sample_embedded_x_ngrams = []\n",
        "for ngram in sample_x_ngrams:\n",
        "  embedded_x_ngram = []\n",
        "  for token in ngram:\n",
        "    if token not in vocab:\n",
        "      embedded_x_ngram.append(vocab['OOV'])\n",
        "    else:\n",
        "      embedded_x_ngram.append(vocab[token])\n",
        "  sample_embedded_x_ngrams.append(embedded_x_ngram)"
      ],
      "metadata": {
        "id": "z7TOdicKIHW3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sample_sentences = np.array(sample_embedded_x_ngrams)\n",
        "sample_y_sentences = np.array(sample_y_ngrams)"
      ],
      "metadata": {
        "id": "2BHOAjD2AlcW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Predicting the tags\n",
        "sample_prediction=model.predict(sample_sentences)"
      ],
      "metadata": {
        "id": "zz1OaDxbAloX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Having a look at the accuracy of the sample sentences\n",
        "sample_correct_predictions = 0\n",
        "for i in range(len(sample_prediction)):\n",
        "  if(compare(sample_prediction[i], sample_y_sentences[i])):\n",
        "    sample_correct_predictions += 1\n",
        "\n",
        "print(f\"Accuracy on the sample sentences is: {sample_correct_predictions / len(sample_prediction):.4f}%\")"
      ],
      "metadata": {
        "id": "3FFi2BrAAlr6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# This is the inverse dictionary of 'd' dictionary, that is - 0 --> BOS like dictionary\n",
        "inverse_d={}\n",
        "for key, value in d.items():\n",
        "  inverse_d[value]=key"
      ],
      "metadata": {
        "id": "9-v2vZV7BAZ5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# The predicted tags are:\n",
        "[inverse_d[np.argmax(sample_prediction[i])] for i in range(len(sample_prediction))]"
      ],
      "metadata": {
        "id": "hbsbPljiBAdS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# The true y's were these\n",
        "[inverse_d[i] for i in [10,8,8,8,8,8,8,8,8,3,7,8,8,10,3,8,8,8,8,3,7,10,8,8,8,3,0,8]]"
      ],
      "metadata": {
        "id": "UyeaVgN9BNJU"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}