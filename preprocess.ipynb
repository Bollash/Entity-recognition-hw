{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "preprocess.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Bollash/Entity-recognition-hw/blob/main/preprocess.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aHJIXfw_4fk8"
      },
      "source": [
        "import pandas as pd\n",
        "import urllib.request\n",
        "import gzip\n",
        "import shutil\n",
        "import numpy as np\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from sklearn.model_selection import train_test_split"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1HPGeP9I96kx",
        "outputId": "61a37dc6-878e-431e-b81f-164e276e4bd3"
      },
      "source": [
        "#Downloading the dataset.\n",
        "url = \"http://hlt.sztaki.hu/resources/hunnerwiki/huwiki.1.ner.tsv.gz\"\n",
        "local_file = \"data.tsv.gz\"\n",
        "urllib.request.urlretrieve(url, local_file)"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('data.tsv.gz', <http.client.HTTPMessage at 0x7f52eba47590>)"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vp7tYTYV_djp"
      },
      "source": [
        "#Decompressing the data\n",
        "with gzip.open(local_file, 'rb') as f_in:\n",
        "    with open('file.tsv', 'wb') as f_out:\n",
        "        shutil.copyfileobj(f_in, f_out)"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OZc6m9C1FzsD",
        "outputId": "fe26d4f9-ec1c-419c-8881-719ae616435c"
      },
      "source": [
        "#Loading the data into data_set. There are lines that produce errors. We filter them out by using the third parameter\n",
        "data_set = pd.read_csv('file.tsv', sep='\\t', error_bad_lines=False)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "b'Skipping line 121529: expected 6 fields, saw 16498\\nSkipping line 121533: expected 6 fields, saw 10\\nSkipping line 121537: expected 6 fields, saw 8198\\n'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LvLf3ACRHN08",
        "outputId": "f3d1d24f-178f-4956-cad8-937a73f665a7"
      },
      "source": [
        "data_set.head"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<bound method NDFrame.head of                      A  text  0                           ART             a  O\n",
              "0               céljuk  text  0              NOUN<POSS<PLUR>>           cél  O\n",
              "1                    ,  text  0                         PUNCT             ,  O\n",
              "2                 hogy  text  0                          CONJ          hogy  O\n",
              "3          biztosítsák  text  0  VERB<SUBJUNC-IMP><PLUR><DEF>      biztosít  O\n",
              "4                    ,  text  0                         PUNCT             ,  O\n",
              "...                ...   ... ..                           ...           ... ..\n",
              "2237028            280  text  0                           NUM           280  O\n",
              "2237029           km/h  text  0                          NOUN          km/h  O\n",
              "2237030              a  text  0                           ART             a  O\n",
              "2237031  végsebbessége  text  0                    NOUN<POSS>  végsebbesség  O\n",
              "2237032              .  text  0                         PUNCT             .  O\n",
              "\n",
              "[2237033 rows x 6 columns]>"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_3aAIBz6MPYA"
      },
      "source": [
        "#We only need the lemmas and the tags. We believe we can extract the entities with the lemmas, and this way we don't have to tokenize, meaning that we have fewer input tokens.\n",
        "smaller = data_set[['a','O']]\n",
        "#Using a fraction of the original data since there are 2.3 million elements.\n",
        "smaller = smaller[0:500000]"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0H2nU7hzcGAJ"
      },
      "source": [
        "#Some lines are NaN. We filter them out this way.\n",
        "smaller = smaller[smaller['O'].notnull()]"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I_Ig1HT1Z5nh"
      },
      "source": [
        "#To convert the tags into numbers we'll use a dictionary. Currently there is no 'PAD', but we shall add them later.\n",
        "d = {\n",
        "    'B-LOC' : 0,\n",
        "    'B-MISC' : 1,\n",
        "    'B-ORG' : 2,\n",
        "    'B-PER' : 3,\n",
        "    'I-LOC' : 4,\n",
        "    'I-MISC' : 5,\n",
        "    'I-ORG' : 6,\n",
        "    'I-PER' : 7,\n",
        "    'O' : 8,\n",
        "    'PAD' : 9\n",
        "}\n",
        "#Converting the tags\n",
        "for idx, row in smaller.iterrows():\n",
        "  row[1] = d[row[1]]"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uK7pCmTaOGew"
      },
      "source": [
        "#Reconstructing the sentences. Every \".\" marks the beginning of a new sentence. This might erroneously produce sentences for \"...\". Should that prove to be a significant issue, we shall change it.\n",
        "sentences = []\n",
        "y_sentences = []\n",
        "sent = []\n",
        "y_sent = []\n",
        "for idx, row in smaller.iterrows():\n",
        "  sent.append(row[0])\n",
        "  y_sent.append(row[1])\n",
        "  if row[0] is '.':\n",
        "    sentences.append(sent)\n",
        "    sent = []\n",
        "    y_sentences.append(y_sent)\n",
        "    y_sent = []"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mSOTqR0ZTN5B"
      },
      "source": [
        "#We need to pad the sentences, so that each input is the same length.\n",
        "max_len = len(max(sentences, key=len))\n",
        "#Pad sentences and y\n",
        "for sentence in sentences:\n",
        "  for _ in range(max_len - len(sentence)):\n",
        "    sentence.append('PAD')\n",
        "for y in y_sentences:\n",
        "  for _ in range(max_len - len(y)):\n",
        "    y.append(9) #9 is our pad symbol"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NfUrykqx3OGe"
      },
      "source": [
        "#One-Hot encode our tags\n",
        "for i in range(len(y_sentences)):\n",
        "  y_sentences[i] = to_categorical(y_sentences[i], 10)"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8Hhx9V6HPxjn"
      },
      "source": [
        "#Converting the lists into Numpy arrays\n",
        "sentences = np.array(sentences)\n",
        "y_sentences = np.array(y_sentences)"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CrEeF7Lh6vRg"
      },
      "source": [
        "#Split the data into train, test, and val data\n",
        "#train : 0.6\n",
        "#test : 0.2\n",
        "#val: 0.2\n",
        "#Since the train test split splits the data into 2 parts we have to call it twice\n",
        "x_train, x_test, y_train, y_test = train_test_split(sentences, y_sentences, test_size=0.2, random_state=123)\n",
        "#We need test_size=0.25 since 0.2 / 0.8 == 0.25\n",
        "x_train, x_val, y_train, y_val = train_test_split(x_train, y_train, test_size=0.25, random_state=123)"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Bir0Ywc5tnfd",
        "outputId": "45e80715-e9c4-49e5-b84a-ad41441e222c"
      },
      "source": [
        "#As you can see, our input data is a 128 length list, containing the tokens that make up the language at the beginning.\n",
        "#They're followed by a rather large amount of \"PAD\" tokens, which are necessary to ensure that every one of our inputs is the same length.\n",
        "#We could use this input with a Bag of Words model (where every word is represented by a number, and in the first stage we transform the words into the numbers assigned to them), but an encoder layer will be more suitable\n",
        "print(x_train[0])\n",
        "\n",
        "#Our y data consists of 128 10 length vector, that's One-Hot encoded.\n",
        "print(y_train[0])"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['a' 'viszonylag' 'vékony' 'profilú' '(' 'nagysebesség' ')' 'hordfelület'\n",
            " '(' 'szárny' ')' 'belépőéle' 'a' 'teljes' 'fesztáv' 'mentén' 'lehajt' ','\n",
            " 'a' 'kilépőél' 'kétharmada' 'egymás' 'csúszik' 'kétrészes' 'fékszárny'\n",
            " 'van' 'ellát' '.' 'PAD' 'PAD' 'PAD' 'PAD' 'PAD' 'PAD' 'PAD' 'PAD' 'PAD'\n",
            " 'PAD' 'PAD' 'PAD' 'PAD' 'PAD' 'PAD' 'PAD' 'PAD' 'PAD' 'PAD' 'PAD' 'PAD'\n",
            " 'PAD' 'PAD' 'PAD' 'PAD' 'PAD' 'PAD' 'PAD' 'PAD' 'PAD' 'PAD' 'PAD' 'PAD'\n",
            " 'PAD' 'PAD' 'PAD' 'PAD' 'PAD' 'PAD' 'PAD' 'PAD' 'PAD' 'PAD' 'PAD' 'PAD'\n",
            " 'PAD' 'PAD' 'PAD' 'PAD' 'PAD' 'PAD' 'PAD' 'PAD' 'PAD' 'PAD' 'PAD' 'PAD'\n",
            " 'PAD' 'PAD' 'PAD' 'PAD' 'PAD' 'PAD' 'PAD' 'PAD' 'PAD' 'PAD' 'PAD' 'PAD'\n",
            " 'PAD' 'PAD' 'PAD' 'PAD' 'PAD' 'PAD' 'PAD' 'PAD' 'PAD' 'PAD' 'PAD' 'PAD'\n",
            " 'PAD' 'PAD' 'PAD' 'PAD' 'PAD' 'PAD' 'PAD' 'PAD' 'PAD' 'PAD' 'PAD' 'PAD'\n",
            " 'PAD' 'PAD' 'PAD' 'PAD' 'PAD' 'PAD' 'PAD']\n",
            "[[0. 0. 0. ... 0. 1. 0.]\n",
            " [0. 0. 0. ... 0. 1. 0.]\n",
            " [0. 0. 0. ... 0. 1. 0.]\n",
            " ...\n",
            " [0. 0. 0. ... 0. 0. 1.]\n",
            " [0. 0. 0. ... 0. 0. 1.]\n",
            " [0. 0. 0. ... 0. 0. 1.]]\n"
          ]
        }
      ]
    }
  ]
}